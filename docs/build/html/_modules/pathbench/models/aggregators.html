<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>pathbench.models.aggregators &mdash; PathBench 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=01f34227"></script>
        <script src="../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            PathBench
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmarking_mode.html">Benchmarking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optimization_mode.html">Optimization Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../visualization_application.html">Visualization Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extending_pathbench.html">Extending PathBench</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_extractors.html">Feature Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mil_aggregators.html">MIL Aggregators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../modules.html">Modules</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PathBench</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">pathbench.models.aggregators</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for pathbench.models.aggregators</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Here one can use pytorch modules as custom MIL aggregator models instead of the ones included in slideflow.</span>
<span class="sd">The construted modules can be imported into the benchmark.py script to be used in the benchmarking process.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">TransformerEncoder</span><span class="p">,</span> <span class="n">TransformerEncoderLayer</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>

<div class="viewcode-block" id="get_activation_function">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.get_activation_function">[docs]</a>
<span class="k">def</span> <span class="nf">get_activation_function</span><span class="p">(</span><span class="n">activation_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return the corresponding activation function from a string name.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">    - activation_name: Name of the activation function</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    - activation_function: Activation function</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">activation_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">activation_name</span><span class="p">)()</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported activation function: </span><span class="si">{</span><span class="n">activation_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="build_encoder">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.build_encoder">[docs]</a>
<span class="k">def</span> <span class="nf">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">use_batchnorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Builds an encoder with a specified number of layers and activation functions.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - use_batchnorm: Whether to use batch normalization in the encoder</span>

<span class="sd">    Returns:</span>
<span class="sd">    - encoder: Encoder network</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">encoder_layers</span><span class="p">):</span>
        <span class="n">in_features</span> <span class="o">=</span> <span class="n">n_feats</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">z_dim</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">activation_function</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_activation_function</span><span class="p">(</span><span class="n">activation_function</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">use_batchnorm</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dropout_p</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span></div>


<div class="viewcode-block" id="linear_evaluation_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.linear_evaluation_mil">[docs]</a>
<span class="k">class</span> <span class="nc">linear_evaluation_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Linear Evaluation MIL model. The model passes the instance embeddings through a linear layer and then</span>
<span class="sd">    through a classifier, while not using any pooling operation or activation function. Therefore useful</span>
<span class="sd">    for linear evaluation of the instance embeddings.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="linear_evaluation_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.linear_evaluation_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">encoder_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="linear_evaluation_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.linear_evaluation_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="linear_evaluation_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.linear_evaluation_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="mean_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.mean_mil">[docs]</a>
<span class="k">class</span> <span class="nc">mean_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mean-pooling MIL model. The model computes the mean pooling of the instance embeddings and passes the result</span>
<span class="sd">    through a classifier.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="mean_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.mean_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="mean_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.mean_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="mean_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.mean_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="max_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.max_mil">[docs]</a>
<span class="k">class</span> <span class="nc">max_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max-pooling MIL model. The model computes the max pooling of the instance embeddings and passes the result</span>
<span class="sd">    through a classifier.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="max_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.max_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="max_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.max_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.max_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="lse_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lse_mil">[docs]</a>
<span class="k">class</span> <span class="nc">lse_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log-sum-exp MIL model. The model computes the log-sum-exp pooling of the instance embeddings and passes the result</span>
<span class="sd">    through a classifier.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - r: Scaling factor for the log-sum-exp pooling</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="lse_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lse_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">r</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="lse_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lse_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="lse_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lse_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">lse_pooling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">embeddings</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">lse_pooling</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="lstm_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lstm_mil">[docs]</a>
<span class="k">class</span> <span class="nc">lstm_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    LSTM MIL model. The model uses an LSTM to process the instance embeddings and passes the final hidden state</span>
<span class="sd">    through a classifier. </span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - lstm_dim: Dimension of the LSTM hidden state</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">use_lens</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="lstm_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lstm_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">lstm_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> 
                 <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">lstm_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">lstm_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">lstm_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="lstm_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lstm_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="lstm_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lstm_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">lens_cpu</span> <span class="o">=</span> <span class="n">lens</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">packed_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">lens_cpu</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">enforce_sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">packed_output</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">packed_embeddings</span><span class="p">)</span>
        <span class="n">lstm_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">packed_output</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="deepset_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.deepset_mil">[docs]</a>
<span class="k">class</span> <span class="nc">deepset_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Deep Sets MIL model. The model uses three fully connected layers to process the instance embeddings:</span>
<span class="sd">    the encoder network, phi and rho. The phi network processes the instance embeddings, which are summed and then the rho network</span>
<span class="sd">    classies the summed embeddings. </span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="deepset_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.deepset_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deepset_phi</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">get_activation_function</span><span class="p">(</span><span class="n">activation_function</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deepset_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">get_activation_function</span><span class="p">(</span><span class="n">activation_function</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="deepset_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.deepset_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="deepset_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.deepset_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">phi_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepset_phi</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">phi_output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepset_rho</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="distributionpooling_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.distributionpooling_mil">[docs]</a>
<span class="k">class</span> <span class="nc">distributionpooling_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Distribution Pooling MIL model. The model computes the mean and variance of the instance embeddings and</span>
<span class="sd">    concatenates these statistics. The model then passes the concatenated embeddings through a classifier.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="distributionpooling_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.distributionpooling_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="distributionpooling_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.distributionpooling_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="distributionpooling_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.distributionpooling_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mean_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">variance_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">mean_embeddings</span><span class="p">,</span> <span class="n">variance_embeddings</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="dsmil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil">[docs]</a>
<span class="k">class</span> <span class="nc">dsmil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dual-stream MIL model. The model operates in two streams: instance-level and bag-level.</span>
<span class="sd">    The instance-level stream evaluates critical instances determined by the instance classifier, which is used</span>
<span class="sd">    to calculate the attention weights for each instance. The bag-level stream computes the bag embeddings</span>
<span class="sd">    using the attention-weighted instance embeddings. The bag embeddings are processed by a convolutional layer</span>
<span class="sd">    and passed through a classifier.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - calculate_attention: Calculate the attention weights for each instance</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="dsmil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bag_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="dsmil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="dsmil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">instance_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">instance_features</span> <span class="o">=</span> <span class="n">instance_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">instance_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_classifier</span><span class="p">(</span><span class="n">instance_features</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">)</span>
        <span class="n">max_score</span><span class="p">,</span> <span class="n">max_idx</span> <span class="o">=</span> <span class="n">instance_scores</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">critical_instance</span> <span class="o">=</span> <span class="n">bags</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">max_idx</span><span class="p">]</span>
        <span class="n">critical_embeddings</span> <span class="o">=</span> <span class="n">instance_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">max_idx</span><span class="p">]</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">instance_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">critical_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">bag_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention_weights</span> <span class="o">*</span> <span class="n">instance_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">bag_embeddings</span> <span class="o">=</span> <span class="n">bag_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">conv_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">bag_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bag_classifier</span><span class="p">(</span><span class="n">conv_out</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span></div>


<div class="viewcode-block" id="dsmil.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">instance_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">instance_embeddings</span> <span class="o">=</span> <span class="n">instance_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">instance_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_classifier</span><span class="p">(</span><span class="n">instance_features</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">)</span>
        <span class="n">max_score</span><span class="p">,</span> <span class="n">max_idx</span> <span class="o">=</span> <span class="n">instance_scores</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">critical_embeddings</span> <span class="o">=</span> <span class="n">instance_embeddings</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">max_idx</span><span class="p">]</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">instance_embeddings</span> <span class="o">-</span> <span class="n">critical_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_scores</span></div>
</div>


<div class="viewcode-block" id="varmil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil">[docs]</a>
<span class="k">class</span> <span class="nc">varmil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Variance MIL model. The model computes the variance and mean of the attention-weighted instance embeddings. </span>
<span class="sd">    The model then concatenates this mean and variance and passes the result through a classifier.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>
<span class="sd">    - calculate_attention: Calculate the attention weights for each instance</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="varmil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="varmil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="varmil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mean_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">embeddings</span> <span class="o">*</span> <span class="n">attention_weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">var_embeddings</span> <span class="o">=</span> <span class="p">((</span><span class="n">embeddings</span> <span class="o">-</span> <span class="n">mean_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">attention_weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">aggregated_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">mean_embeddings</span><span class="p">,</span> <span class="n">var_embeddings</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">aggregated_embeddings</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span></div>


<div class="viewcode-block" id="varmil.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_scores</span></div>
</div>


<div class="viewcode-block" id="perceiver_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.perceiver_mil">[docs]</a>
<span class="k">class</span> <span class="nc">perceiver_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perceiver MIL model. The model uses a learnable latent array to compute attention weights between the input features</span>
<span class="sd">    and the latent array. The model then applies transformer layers to the latent array and uses the CLS token for classification.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - latent_dim: Dimension of the latent array</span>
<span class="sd">    - num_latents: Number of learnable latents</span>
<span class="sd">    - num_layers: Number of transformer layers</span>
<span class="sd">    - num_heads: Number of attention heads</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - output: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="perceiver_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.perceiver_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> 
                 <span class="n">num_latents</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> 
                 <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">perceiver_mil</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># Introduce a CLS token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latents</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_latents</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">)</span>
        
        <span class="c1"># Transformer layers after cross-attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_layers</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span>
            <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="o">=</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span>
        <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="perceiver_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.perceiver_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="perceiver_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.perceiver_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1"># Project input features to latent dimension</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Combine CLS token with latent space</span>
        <span class="n">cls_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, 1, latent_dim)</span>
        <span class="n">latents</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latents</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, 1 + num_latents, latent_dim)</span>
        
        <span class="c1"># Cross-attention between input features and learnable latent array</span>
        <span class="n">latents</span> <span class="o">=</span> <span class="n">latents</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (1 + num_latents, batch_size, latent_dim)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (n_patches, batch_size, latent_dim)</span>
        <span class="n">latents</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">latents</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Apply transformer layers to the latents</span>
        <span class="n">latents</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_layers</span><span class="p">(</span><span class="n">latents</span><span class="p">)</span>
        
        <span class="c1"># Use the CLS token directly for classification</span>
        <span class="n">cls_output</span> <span class="o">=</span> <span class="n">latents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># (batch_size, latent_dim)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span><span class="p">(</span><span class="n">cls_output</span><span class="p">)</span>  <span class="c1"># (batch_size, n_out)</span>
        <span class="k">return</span> <span class="n">output</span></div>
</div>


<div class="viewcode-block" id="gated_attention_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.gated_attention_mil">[docs]</a>
<span class="k">class</span> <span class="nc">gated_attention_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gated Attention MIL model. The model computes attention weights for each instance and aggregates the instance</span>
<span class="sd">    embeddings using the attention weights in a gated manner. The model then passes the aggregated embeddings through a classifier.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - calculate_attention: Calculate the attention weights for each instance</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="gated_attention_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.gated_attention_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="gated_attention_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.gated_attention_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="gated_attention_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.gated_attention_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">attention_U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span><span class="p">(</span><span class="n">attention_V</span> <span class="o">*</span> <span class="n">attention_U</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">embeddings</span> <span class="o">*</span> <span class="n">attention_weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>


<div class="viewcode-block" id="gated_attention_mil.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.gated_attention_mil.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">attention_U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span><span class="p">(</span><span class="n">attention_V</span> <span class="o">*</span> <span class="n">attention_U</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_weights</span></div>
</div>


<div class="viewcode-block" id="topk_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.topk_mil">[docs]</a>
<span class="k">class</span> <span class="nc">topk_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Top-k MIL model. The model computes attention weights for each instance and selects the top-k instances</span>
<span class="sd">    based on these weights. The model then computes the weighted sum of the top-k instances and passes the result</span>
<span class="sd">    through a classifier. Useful for problems where the number of relevant instances can be estimated.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - calculate_attention: Calculate the attention weights for each instance</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="topk_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.topk_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> 
                 <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="topk_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.topk_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="topk_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.topk_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">)</span>
        <span class="n">topk_scores</span><span class="p">,</span> <span class="n">topk_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">topk_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">topk_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        
        <span class="c1"># Apply softmax to top-k scores to get attention weights</span>
        <span class="n">topk_attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">topk_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Compute the weighted sum of the top-k embeddings</span>
        <span class="n">weighted_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">topk_embeddings</span> <span class="o">*</span> <span class="n">topk_attention_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">weighted_sum</span><span class="p">),</span> <span class="n">topk_attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">weighted_sum</span><span class="p">)</span></div>


<div class="viewcode-block" id="topk_mil.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.topk_mil.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="air_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.air_mil">[docs]</a>
<span class="k">class</span> <span class="nc">air_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adaptive Instance Ranking MIL (AIR-MIL), which is a learnable top-k MIL model. First, the model computes</span>
<span class="sd">    attention weights for each instance and selects the top-k instances based on these weights. The model then</span>
<span class="sd">    computes the weighted mean of the top-k instances and passes the result through a classifier. The model also</span>
<span class="sd">    includes a learnable parameter k that determines the number of instances to select. Useful for problems</span>
<span class="sd">    where the number of relevant intances is unknown.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - initial_k: Initial value of the learnable parameter k</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - calculate_attention: Calculate the attention weights for each instance</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="air_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.air_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">initial_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> 
                 <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Always learnable k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_param</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">initial_k</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="air_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.air_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="air_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.air_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Bound k to be between 1 and n_patches</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k_param</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        
        <span class="n">topk_scores</span><span class="p">,</span> <span class="n">topk_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">topk_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">topk_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        
        <span class="c1"># Apply softmax to top-k scores to get attention weights</span>
        <span class="n">topk_attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">topk_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Compute the weighted mean of the top-k embeddings</span>
        <span class="n">weighted_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">topk_embeddings</span> <span class="o">*</span> <span class="n">topk_attention_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">weighted_mean</span> <span class="o">=</span> <span class="n">weighted_sum</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">topk_attention_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">weighted_mean</span><span class="p">),</span> <span class="n">topk_attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">weighted_mean</span><span class="p">)</span></div>


<div class="viewcode-block" id="air_mil.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.air_mil.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>




<div class="viewcode-block" id="il_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.il_mil">[docs]</a>
<span class="k">class</span> <span class="nc">il_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Instance-level MIL model. The model classifies each instance and aggregates the instance predictions</span>
<span class="sd">    using the mean operation. Useful for cases where a small number of relevant instances are present in each bag.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - bag_scores: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="il_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.il_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="il_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.il_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="il_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.il_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># Flatten bags to pass through the encoder</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="c1"># Reshape to original bag structure</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Classify each instance</span>
        <span class="n">instance_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_classifier</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="c1"># Aggregate instance predictions (mean)</span>
        <span class="n">bag_scores</span> <span class="o">=</span> <span class="n">instance_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">bag_scores</span></div>
</div>


<div class="viewcode-block" id="weighted_mean_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.weighted_mean_mil">[docs]</a>
<span class="k">class</span> <span class="nc">weighted_mean_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Weighted mean MIL model. The variance of the instance embeddings is used to compute instance weights.</span>
<span class="sd">    The instance embeddings are then aggregated using the computed weights. Useful for noisy data.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - output: Predicted class scores for in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="weighted_mean_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.weighted_mean_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span>
    <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">weighted_mean_mil</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="weighted_mean_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.weighted_mean_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="weighted_mean_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.weighted_mean_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">variances</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
        <span class="n">weighted_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">robust_mean</span> <span class="o">=</span> <span class="n">weighted_sum</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">robust_mean</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div>
</div>



<div class="viewcode-block" id="clam_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil">[docs]</a>
<span class="k">class</span> <span class="nc">clam_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Clusering-constrained Attention MIL (CLAM-MIL) model. The model uses a learnable attention mechanism to</span>
<span class="sd">    aggregate instance embeddings into slide-level representations. The model also includes a clustering module</span>
<span class="sd">    to predict instance-level cluster assignments. For each output class the model has a separate attention</span>
<span class="sd">    mechanism and classifier. </span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - cluster_patches: Cluster patches into clusters</span>
<span class="sd">    - calculate_attention: Calculate the attention weights for each instance</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for in the bag</span>
<span class="sd">    - attention_weights: Attention weights for each instance (optional)</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="clam_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span>
    <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">clam_mil</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_out</span> <span class="o">=</span> <span class="n">n_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifiers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_cluster</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="clam_mil._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="clam_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">slide_level_representations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">attention_weights_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
            <span class="n">attention_U_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">attention_V_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">attention_U_output</span> <span class="o">*</span> <span class="n">attention_V_output</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attention_weights_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
            <span class="n">slide_level_representation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attention_scores</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">slide_level_representations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slide_level_representation</span><span class="p">)</span>

        <span class="n">slide_level_representations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">slide_level_representations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifiers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">slide_level_representations</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Stack the attention weights and then take the mean along the first dimension (across branches)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">attention_weights_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Shape: (n_out, batch_size, n_patches, 1)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, n_patches, 1)</span>

        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span></div>


<div class="viewcode-block" id="clam_mil.cluster_patches">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil.cluster_patches">[docs]</a>
    <span class="k">def</span> <span class="nf">cluster_patches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">cluster_predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
            <span class="n">cluster_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_cluster</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">cluster_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cluster_pred</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cluster_predictions</span></div>


<div class="viewcode-block" id="clam_mil.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_weights_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
            <span class="n">attention_U_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">attention_V_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">attention_U_output</span> <span class="o">*</span> <span class="n">attention_V_output</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attention_weights_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>

        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">attention_weights_list</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_weights</span></div>
</div>



<div class="viewcode-block" id="clam_mil_mb">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb">[docs]</a>
<span class="k">class</span> <span class="nc">clam_mil_mb</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CLAM-MIL model with multiple branches, each with its own attention mechanism and classifier.</span>

<span class="sd">    Args:</span>
<span class="sd">    - n_feats: Number of input features</span>
<span class="sd">    - n_out: Number of output classes</span>
<span class="sd">    - z_dim: Dimension of the latent space</span>
<span class="sd">    - dropout_p: Dropout probability</span>
<span class="sd">    - n_branches: Number of branches</span>
<span class="sd">    - activation_function: Activation function to use in the encoder</span>
<span class="sd">    - encoder_layers: Number of layers in the encoder</span>

<span class="sd">    Methods:</span>
<span class="sd">    - forward: Forward pass through the model</span>
<span class="sd">    - calculate_attention: Calculate the attention weights for each instance</span>
<span class="sd">    - initialize_weights: Initialize the weights of the model</span>

<span class="sd">    Returns:</span>
<span class="sd">    - scores: Predicted class scores for instances in the bag</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="clam_mil_mb.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">n_branches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span>
    <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">clam_mil_mb</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_out</span> <span class="o">=</span> <span class="n">n_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_branches</span> <span class="o">=</span> <span class="n">n_branches</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">build_encoder</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="p">,</span> <span class="n">activation_function</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifiers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_cluster</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_weights</span><span class="p">()</span></div>


<div class="viewcode-block" id="clam_mil_mb._initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb._initialize_weights">[docs]</a>
    <span class="k">def</span> <span class="nf">_initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">initialize_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="clam_mil_mb.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="n">all_scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># Process each branch separately</span>
        <span class="k">for</span> <span class="n">branch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_branches</span><span class="p">):</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">](</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>  
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="n">slide_level_representations</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">attention_weights_list</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
                <span class="n">attention_U_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
                <span class="n">attention_V_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
                <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">][</span><span class="n">i</span><span class="p">](</span><span class="n">attention_U_output</span> <span class="o">*</span> <span class="n">attention_V_output</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">attention_weights_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
                <span class="n">slide_level_representation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attention_scores</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">slide_level_representations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slide_level_representation</span><span class="p">)</span>

            <span class="n">slide_level_representations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">slide_level_representations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
                <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifiers</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">][</span><span class="n">i</span><span class="p">](</span><span class="n">slide_level_representations</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
                <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attention_weights_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">all_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
            <span class="n">all_attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>

        <span class="c1"># Combine the results from all branches</span>
        <span class="n">combined_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_scores</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">combined_attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_attention_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">combined_scores</span><span class="p">,</span> <span class="n">combined_attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">combined_scores</span></div>


<div class="viewcode-block" id="clam_mil_mb.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="n">all_attention_weights</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Process each branch separately</span>
        <span class="k">for</span> <span class="n">branch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_branches</span><span class="p">):</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">](</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="n">attention_weights_list</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
                <span class="n">attention_U_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
                <span class="n">attention_V_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
                <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span><span class="p">[</span><span class="n">branch_idx</span><span class="p">][</span><span class="n">i</span><span class="p">](</span><span class="n">attention_U_output</span> <span class="o">*</span> <span class="n">attention_V_output</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
                    <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">attention_weights_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>

            <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attention_weights_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">all_attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>

        <span class="c1"># Combine the attention weights from all branches</span>
        <span class="n">combined_attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_attention_weights</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">combined_attention_weights</span></div>
</div>



<div class="viewcode-block" id="initialize_weights">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.initialize_weights">[docs]</a>
<span class="k">def</span> <span class="nf">initialize_weights</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the weights of the model using Xavier initialization for linear layers and constant initialization</span>
<span class="sd">    for batch normalization layers.</span>

<span class="sd">    Args:</span>
<span class="sd">    - module: The model to initialize</span>

<span class="sd">    Returns:</span>
<span class="sd">    - None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">):</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Siemen Brussee.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>