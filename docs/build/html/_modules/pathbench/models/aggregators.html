<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>pathbench.models.aggregators &mdash; PathBench 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=01f34227"></script>
        <script src="../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            PathBench
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../benchmarking_mode.html">Benchmarking Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optimization_mode.html">Optimization Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../visualization_application.html">Visualization Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ensemble_model_creation.html">Ensemble Model Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../extending_pathbench.html">Extending PathBench</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../feature_extractors.html">Feature Extractors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mil_aggregators.html">MIL Aggregators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../modules.html">Modules</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">PathBench</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">pathbench.models.aggregators</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for pathbench.models.aggregators</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Here one can use pytorch modules as custom MIL aggregator models instead of the ones included in slideflow.</span>
<span class="sd">The construted modules can be imported into the benchmark.py script to be used in the benchmarking process.</span>
<span class="sd">As an example, we added some simple MIL methods (linear + mean, linear + max) below.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>


<div class="viewcode-block" id="simple_linear_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.simple_linear_mil">[docs]</a>
<span class="k">class</span> <span class="nc">simple_linear_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple Multiple instance learning model with linear layers, useful for linear evaluation.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    &quot;&quot;&quot;</span>


<div class="viewcode-block" id="simple_linear_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.simple_linear_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_out</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span></div>


<div class="viewcode-block" id="simple_linear_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.simple_linear_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>

        

<div class="viewcode-block" id="linear_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.linear_mil">[docs]</a>
<span class="k">class</span> <span class="nc">linear_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with linear layers.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="linear_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.linear_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="linear_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.linear_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="mean_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.mean_mil">[docs]</a>
<span class="k">class</span> <span class="nc">mean_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with mean pooling.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="mean_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.mean_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="mean_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.mean_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>



<div class="viewcode-block" id="max_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.max_mil">[docs]</a>
<span class="k">class</span> <span class="nc">max_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with max pooling.</span>
<span class="sd">    </span>
<span class="sd">        Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="max_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.max_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="max_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.max_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="lse_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lse_mil">[docs]</a>
<span class="k">class</span> <span class="nc">lse_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with log-sum-exp pooling.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    r : float</span>
<span class="sd">        scaling factor for log-sum-exp pooling</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    r : float</span>
<span class="sd">        scaling factor for log-sum-exp pooling</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="lse_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lse_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">r</span></div>


<div class="viewcode-block" id="lse_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lse_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">lse_pooling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">embeddings</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">lse_pooling</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>



<div class="viewcode-block" id="lstm_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lstm_mil">[docs]</a>
<span class="k">class</span> <span class="nc">lstm_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with LSTM-based pooling.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    lstm_dim : int</span>
<span class="sd">        Dimensionality of the LSTM hidden state</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    lstm : nn.LSTM</span>
<span class="sd">        LSTM network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    </span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">use_lens</span><span class="o">=</span><span class="kc">True</span>

<div class="viewcode-block" id="lstm_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lstm_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">lstm_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">lstm_dim</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">lstm_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">lstm_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="lstm_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.lstm_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, num_instances, z_dim)</span>
        <span class="n">lens_cpu</span> <span class="o">=</span> <span class="n">lens</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>  <span class="c1"># Ensure lengths are on CPU and of type int64 for packing</span>
        <span class="n">packed_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">lens_cpu</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">enforce_sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">packed_output</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">packed_embeddings</span><span class="p">)</span>
        <span class="n">lstm_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">packed_output</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># Use the last hidden state of the LSTM as the pooled representation</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="deepset_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.deepset_mil">[docs]</a>
<span class="k">class</span> <span class="nc">deepset_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiple instance learning model with DeepSet-based pooling.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    deepset_phi : nn.Sequential</span>
<span class="sd">        DeepSet phi network</span>
<span class="sd">    deepset_rho : nn.Sequential</span>
<span class="sd">        DeepSet rho network</span>
<span class="sd">    </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="deepset_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.deepset_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deepset_phi</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deepset_rho</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="deepset_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.deepset_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">phi_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepset_phi</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">phi_output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">deepset_rho</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>



<div class="viewcode-block" id="distributionpooling_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.distributionpooling_mil">[docs]</a>
<span class="k">class</span> <span class="nc">distributionpooling_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with distribution pooling (mean and variance).</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="distributionpooling_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.distributionpooling_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Encoder to transform raw features into a higher-dimensional space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="c1"># Head network to produce final classification scores</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>  <span class="c1"># Since we&#39;ll concatenate mean and variance</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="distributionpooling_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.distributionpooling_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="c1"># Encode each instance in the bag</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="c1"># Calculate the mean and variance of the embeddings</span>
        <span class="n">mean_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">variance_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Concatenate the mean and variance</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">mean_embeddings</span><span class="p">,</span> <span class="n">variance_embeddings</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Pass through the head network to get final scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="dsmil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil">[docs]</a>
<span class="k">class</span> <span class="nc">dsmil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dual-Stream Multiple Instance Learning model with attention mechanism and Conv1D bag-level classifier.</span>
<span class="sd">    DSMIL first computes instance-level embeddings and scores, then aggregates them into bag-level embeddings,</span>
<span class="sd">    based on an attention mechanism. Finally, a Conv1D classifier is used to predict the bag-level label.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    instance_encoder : nn.Sequential</span>
<span class="sd">        Instance encoder network</span>
<span class="sd">    instance_classifier : nn.Linear</span>
<span class="sd">        Instance classifier network</span>
<span class="sd">    </span>
<span class="sd">    attention : nn.Sequential</span>
<span class="sd">        Attention network</span>
<span class="sd">    conv1d : nn.Conv1d</span>
<span class="sd">        Conv1D network</span>
<span class="sd">    bag_classifier : nn.Sequential</span>
<span class="sd">        Bag-level classifier network</span>
<span class="sd">    </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags, return_attention)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    </span>
<span class="sd">    calculate_attention(bags, lens, apply_softmax)</span>
<span class="sd">        Calculate attention scores for the given bags</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="dsmil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bag_classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="dsmil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

        <span class="c1"># Instance-level encoding and classification</span>
        <span class="n">instance_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">instance_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_classifier</span><span class="p">(</span><span class="n">instance_features</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">)</span>

        <span class="c1"># Max pooling stream</span>
        <span class="n">max_score</span><span class="p">,</span> <span class="n">max_idx</span> <span class="o">=</span> <span class="n">instance_scores</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">critical_instance</span> <span class="o">=</span> <span class="n">bags</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">max_idx</span><span class="p">]</span>

        <span class="c1"># Attention mechanism</span>
        <span class="n">instance_embeddings</span> <span class="o">=</span> <span class="n">instance_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">critical_embeddings</span> <span class="o">=</span> <span class="n">instance_embeddings</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">max_idx</span><span class="p">]</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">instance_embeddings</span> <span class="o">-</span> <span class="n">critical_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">bag_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">attention_weights</span> <span class="o">*</span> <span class="n">instance_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Bag-level classification</span>
        <span class="n">bag_embeddings</span> <span class="o">=</span> <span class="n">bag_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">conv_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="n">bag_embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bag_classifier</span><span class="p">(</span><span class="n">conv_out</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span></div>



<div class="viewcode-block" id="dsmil.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.dsmil.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        
        <span class="c1"># Instance-level encoding</span>
        <span class="n">instance_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">instance_embeddings</span> <span class="o">=</span> <span class="n">instance_features</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Get critical instance</span>
        <span class="n">instance_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_classifier</span><span class="p">(</span><span class="n">instance_features</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_instances</span><span class="p">)</span>
        <span class="n">max_score</span><span class="p">,</span> <span class="n">max_idx</span> <span class="o">=</span> <span class="n">instance_scores</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">critical_embeddings</span> <span class="o">=</span> <span class="n">instance_embeddings</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">max_idx</span><span class="p">]</span>
        
        <span class="c1"># Compute attention scores</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">instance_embeddings</span> <span class="o">-</span> <span class="n">critical_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">attention_scores</span></div>
</div>



<div class="viewcode-block" id="varmil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil">[docs]</a>
<span class="k">class</span> <span class="nc">varmil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multiple instance learning model with attention-based mean and variance pooling.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    attention : nn.Sequential</span>
<span class="sd">        Attention network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags, return_attention)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">        </span>
<span class="sd">    calculate_attention(bags, lens, apply_softmax)</span>
<span class="sd">        Calculate attention scores for the given bags</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="varmil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="varmil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mean_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">embeddings</span> <span class="o">*</span> <span class="n">attention_weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">var_embeddings</span> <span class="o">=</span> <span class="p">((</span><span class="n">embeddings</span> <span class="o">-</span> <span class="n">mean_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">attention_weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">aggregated_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">mean_embeddings</span><span class="p">,</span> <span class="n">var_embeddings</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">aggregated_embeddings</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span></div>


<div class="viewcode-block" id="varmil.calculate_attention">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.varmil.calculate_attention">[docs]</a>
    <span class="k">def</span> <span class="nf">calculate_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">lens</span><span class="p">,</span> <span class="n">apply_softmax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_scores</span></div>
</div>


<div class="viewcode-block" id="cluster_pooling_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.cluster_pooling_mil">[docs]</a>
<span class="k">class</span> <span class="nc">cluster_pooling_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with cluster-based pooling. It first clusters patches into regions, pools each region using mean pooling</span>
<span class="sd">    and then aggregates region embeddings into the slide-level label using a linear classifier.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    n_clusters : int</span>
<span class="sd">        Number of clusters</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    kmeans : KMeans</span>
<span class="sd">        KMeans clustering model</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="cluster_pooling_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.cluster_pooling_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="n">n_clusters</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span> <span class="o">*</span> <span class="n">n_clusters</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span></div>


<div class="viewcode-block" id="cluster_pooling_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.cluster_pooling_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Process each batch element independently</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Encode each patch in the current batch element</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># Shape: (n_patches, z_dim)</span>

            <span class="c1"># Determine the number of clusters based on the number of patches</span>
            <span class="n">n_clusters</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kmeans</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">n_clusters</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="c1"># If we have fewer than 2 patches, use the mean of all embeddings</span>
                <span class="n">cluster_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">pooled_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cluster_embedding</span><span class="p">)</span>
                <span class="k">continue</span>
            
            <span class="c1"># Perform KMeans clustering for the current batch element</span>
            <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">)</span>
            <span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

            <span class="c1"># Pool embeddings within each cluster</span>
            <span class="n">cluster_embeddings</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
                <span class="n">cluster_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">j</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">embeddings</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">cluster_indices</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">cluster_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">cluster_indices</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Handle the case where a cluster has no elements</span>
                    <span class="n">cluster_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">embeddings</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">cluster_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cluster_embedding</span><span class="p">)</span>
            
            <span class="c1"># Concatenate the pooled embeddings for each cluster</span>
            <span class="n">pooled_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">cluster_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="c1"># Stack the pooled embeddings to form a tensor of shape (batch_size, z_dim * n_clusters)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Pass the pooled embeddings through the prediction head</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, n_out)</span>

        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="gated_attention_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.gated_attention_mil">[docs]</a>
<span class="k">class</span> <span class="nc">gated_attention_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with gated attention mechanism.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    attention_V : nn.Sequential</span>
<span class="sd">        Attention network</span>
<span class="sd">    attention_U : nn.Sequential</span>
<span class="sd">        Attention network</span>
<span class="sd">    attention_weights : nn.Linear</span>
<span class="sd">        Attention network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    </span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>

<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="gated_attention_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.gated_attention_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>



<div class="viewcode-block" id="gated_attention_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.gated_attention_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">attention_V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">attention_U</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span><span class="p">(</span><span class="n">attention_V</span> <span class="o">*</span> <span class="n">attention_U</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">embeddings</span> <span class="o">*</span> <span class="n">attention_weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="topk_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.topk_mil">[docs]</a>
<span class="k">class</span> <span class="nc">topk_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model which selects the top k instances based on attention scores and aggregates them using mean pooling.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    k : int</span>
<span class="sd">        Number of top instances to select</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    attention : nn.Sequential</span>
<span class="sd">        Attention network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    k : int</span>
<span class="sd">        Number of top instances to select</span>
<span class="sd">    </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>

<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="topk_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.topk_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span></div>


<div class="viewcode-block" id="topk_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.topk_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="c1"># Assuming `bags` has shape (batch_size, n_patches, n_feats)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Compute attention scores for each instance</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Select top k instances for each item in the batch</span>
        <span class="n">topk_scores</span><span class="p">,</span> <span class="n">topk_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Gather top k embeddings for each batch item</span>
        <span class="n">topk_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">topk_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        
        <span class="c1"># Mean pooling over the selected top k instances</span>
        <span class="n">pooled_embeddings</span> <span class="o">=</span> <span class="n">topk_embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Pass the pooled embeddings through the prediction head</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">pooled_embeddings</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="hierarchical_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.hierarchical_mil">[docs]</a>
<span class="k">class</span> <span class="nc">hierarchical_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hierarchical Multiple Instance Learning model with attention mechanism. It first groups consecutive patches into regions,</span>
<span class="sd">    weights instances per region using attention and then aggregates region embeddings into slide embeddings using an attention mechanism.</span>
<span class="sd">    Finally, it predicts the slide-level label using a linear classifier.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    region_size : int</span>
<span class="sd">        Number of patches per region</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    region_attention : nn.Sequential</span>
<span class="sd">        Region attention network</span>
<span class="sd">    region_head : nn.Sequential</span>
<span class="sd">        Region head network</span>
<span class="sd">    slide_attention : nn.Sequential</span>
<span class="sd">        Slide attention network</span>
<span class="sd">    slide_head : nn.Sequential</span>
<span class="sd">        Slide head network</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>

<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="hierarchical_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.hierarchical_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">region_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">region_attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">region_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slide_attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slide_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">region_size</span> <span class="o">=</span> <span class="n">region_size</span></div>


<div class="viewcode-block" id="hierarchical_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.hierarchical_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="c1"># Split into regions</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">n_regions</span> <span class="o">=</span> <span class="n">n_patches</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">region_size</span>

        <span class="n">region_embeddings</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_regions</span><span class="p">):</span>
            <span class="n">region_patches</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="n">i</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">region_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">region_size</span><span class="p">]</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">region_attention</span><span class="p">(</span><span class="n">region_patches</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">region_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attention_weights</span> <span class="o">*</span> <span class="n">region_patches</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">region_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">region_head</span><span class="p">(</span><span class="n">region_embedding</span><span class="p">))</span>

        <span class="n">region_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">region_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Attention pooling across regions</span>
        <span class="n">region_attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slide_attention</span><span class="p">(</span><span class="n">region_embeddings</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">slide_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region_attention_weights</span> <span class="o">*</span> <span class="n">region_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Final slide-level prediction</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slide_head</span><span class="p">(</span><span class="n">slide_embedding</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<span class="k">class</span> <span class="nc">hierarchical_cluster_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Inspired by &quot;Cluster-to-Conquer: A Framework for End-to-End</span>
<span class="sd">    Multi-Instance Learning for Whole Slide Image Classification&quot;.</span>

<span class="sd">    Hierarchical Cluster MIL model with attention mechanism. It first clusters patches into regions, weights instances</span>
<span class="sd">    per region using attention and then aggregates region embeddings into slide embeddings using an attention mechanism.</span>
<span class="sd">    Finally, it predicts the slide-level label using a linear classifier.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    n_clusters : int</span>
<span class="sd">        Number of clusters</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    kmeans : KMeans</span>
<span class="sd">        KMeans clustering model</span>
<span class="sd">    region_attention : nn.Sequential</span>
<span class="sd">        Region attention network</span>
<span class="sd">    region_head : nn.Sequential</span>
<span class="sd">        Region head network</span>
<span class="sd">    slide_attention : nn.Sequential</span>
<span class="sd">        Slide attention network</span>
<span class="sd">    slide_head : nn.Sequential</span>
<span class="sd">        Slide head network</span>

<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="hierarchical_cluster_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.hierarchical_cluster_mil">[docs]</a>
<span class="k">class</span> <span class="nc">hierarchical_cluster_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<div class="viewcode-block" id="hierarchical_cluster_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.hierarchical_cluster_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">region_attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">region_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slide_attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slide_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="hierarchical_cluster_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.hierarchical_cluster_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">all_region_embeddings</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Process each batch element independently</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="c1"># Encode patches</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>  <span class="c1"># Shape: (n_patches, z_dim)</span>

            <span class="c1"># Perform KMeans clustering for each item in the batch</span>
            <span class="n">n_clusters</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kmeans</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">n_clusters</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="c1"># If fewer than 2 patches, use the mean of all embeddings</span>
                <span class="n">region_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Shape: (1, z_dim)</span>
                <span class="n">all_region_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">region_embeddings</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="n">clusters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

            <span class="n">region_embeddings</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># Process each cluster within the batch item</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
                <span class="n">cluster_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">j</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">embeddings</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">cluster_patches</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">cluster_mask</span><span class="p">]</span>  <span class="c1"># Shape: (n_cluster_patches, z_dim)</span>

                <span class="k">if</span> <span class="n">cluster_patches</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># If no patches in this cluster</span>
                    <span class="k">continue</span>  <span class="c1"># Skip this cluster</span>

                <span class="c1"># Apply attention within each region</span>
                <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">region_attention</span><span class="p">(</span><span class="n">cluster_patches</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">region_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attention_weights</span> <span class="o">*</span> <span class="n">cluster_patches</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Shape: (z_dim)</span>
                <span class="n">region_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">region_head</span><span class="p">(</span><span class="n">region_embedding</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>  <span class="c1"># Shape: (1, z_dim)</span>

            <span class="k">if</span> <span class="n">region_embeddings</span><span class="p">:</span>
                <span class="n">all_region_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">region_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># Shape: (n_clusters, z_dim)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_region_embeddings</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Stack all region embeddings across the batch</span>
            <span class="n">region_embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_region_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, n_clusters, z_dim)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No regions were found. Check your clustering or input data.&quot;</span><span class="p">)</span>

        <span class="c1"># Ensure that region_embeddings has at least 2 dimensions for BatchNorm</span>
        <span class="k">if</span> <span class="n">region_embeddings</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">region_embeddings</span> <span class="o">=</span> <span class="n">region_embeddings</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Attention pooling across regions within each batch item</span>
        <span class="n">region_attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slide_attention</span><span class="p">(</span><span class="n">region_embeddings</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, n_clusters, 1)</span>
        <span class="n">slide_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region_attention_weights</span> <span class="o">*</span> <span class="n">region_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, z_dim)</span>

        <span class="c1"># Final slide-level prediction</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slide_head</span><span class="p">(</span><span class="n">slide_embedding</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, n_out)</span>
        <span class="k">return</span> <span class="n">scores</span></div>
</div>


<div class="viewcode-block" id="retmil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil">[docs]</a>
<span class="k">class</span> <span class="nc">retmil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Retention-based MIL. Retention mechanism is applied to both local subsequences and the global sequence in a hierarchical manner.</span>
<span class="sd">    The local retention mechanism uses relative distance decay to compute the retention scores, while the global retention mechanism</span>
<span class="sd">    uses self-attention to compute the retention scores. The final prediction is made using a linear classifier.</span>
<span class="sd">    Method from: &quot;RetMIL: Retentive Multiple Instance Learning for Histopathological Whole Slide Image Classification&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="retmil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">subseq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">retmil</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subseq_len</span> <span class="o">=</span> <span class="n">subseq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>

        <span class="c1"># Retention mechanism for local subsequences</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_retention_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">RetentionLayer</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_retention_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">RetentionLayer</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">AttentionPooling</span><span class="p">(</span><span class="n">z_dim</span><span class="p">)</span>

        <span class="c1"># Retention mechanism for global sequence</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_retention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">RetentionLayer</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">AttentionPooling</span><span class="p">(</span><span class="n">z_dim</span><span class="p">)</span>

        <span class="c1"># Classification head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="retmil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="c1"># Step 1: Split into subsequences</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">subsequences</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">bags</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">subseq_len</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Step 2: Local subsequence processing</span>
        <span class="n">local_embeddings</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">subseq</span> <span class="ow">in</span> <span class="n">subsequences</span><span class="p">:</span>
            <span class="n">subseq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_retention_1</span><span class="p">(</span><span class="n">subseq</span><span class="p">)</span>
            <span class="n">subseq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_retention_2</span><span class="p">(</span><span class="n">subseq</span><span class="p">)</span>
            <span class="n">local_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_attention</span><span class="p">(</span><span class="n">subseq</span><span class="p">)</span>
            <span class="n">local_embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">local_embedding</span><span class="p">)</span>

        <span class="c1"># Stack local embeddings into a global sequence</span>
        <span class="n">global_sequence</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">local_embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, num_subseq, z_dim)</span>

        <span class="c1"># Step 3: Global sequence processing</span>
        <span class="n">global_sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_retention</span><span class="p">(</span><span class="n">global_sequence</span><span class="p">)</span>
        <span class="n">global_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_attention</span><span class="p">(</span><span class="n">global_sequence</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, z_dim)</span>

        <span class="c1"># Step 4: Classification</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">global_embedding</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, n_out)</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="retmil.RetentionLayer">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.RetentionLayer">[docs]</a>
    <span class="k">class</span> <span class="nc">RetentionLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<div class="viewcode-block" id="retmil.RetentionLayer.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.RetentionLayer.__init__">[docs]</a>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">retmil</span><span class="o">.</span><span class="n">RetentionLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">WQ</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">WK</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">WV</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">group_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="retmil.RetentionLayer.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.RetentionLayer.forward">[docs]</a>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

            <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WQ</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WK</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WV</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Implementing a simplified version of retention mechanism</span>
            <span class="c1"># Retention using relative distance decay</span>
            <span class="n">D</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relative_distance_decay</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">retention_scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">D</span>
            <span class="n">retention_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">retention_scores</span> <span class="o">@</span> <span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

            <span class="c1"># Applying normalization</span>
            <span class="n">retention_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="n">retention_out</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">retention_out</span></div>


<div class="viewcode-block" id="retmil.RetentionLayer.relative_distance_decay">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.RetentionLayer.relative_distance_decay">[docs]</a>
        <span class="k">def</span> <span class="nf">relative_distance_decay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
            <span class="n">D</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
                    <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="n">j</span> <span class="o">-</span> <span class="n">i</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">D</span></div>
</div>


<div class="viewcode-block" id="retmil.AttentionPooling">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.AttentionPooling">[docs]</a>
    <span class="k">class</span> <span class="nc">AttentionPooling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<div class="viewcode-block" id="retmil.AttentionPooling.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.AttentionPooling.__init__">[docs]</a>
        <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">retmil</span><span class="o">.</span><span class="n">AttentionPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">U</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span></div>


<div class="viewcode-block" id="retmil.AttentionPooling.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.retmil.AttentionPooling.forward">[docs]</a>
        <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span></div>
</div>
</div>


<div class="viewcode-block" id="weighted_mean_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.weighted_mean_mil">[docs]</a>
<span class="k">class</span> <span class="nc">weighted_mean_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiple instance learning model with weighted mean pooling. Instances are inversely weighted by their variance.</span>
<span class="sd">    This effectively makes the model more robust to noisy instances.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">        </span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="weighted_mean_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.weighted_mean_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">weighted_mean_mil</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="weighted_mean_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.weighted_mean_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="c1"># Assume bags has shape (batch_size, n_patches, n_feats)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">n_feats</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        
        <span class="c1"># Encode each patch</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">))</span>  <span class="c1"># Shape: (batch_size * n_patches, z_dim)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reshape to (batch_size, n_patches, z_dim)</span>
        
        <span class="c1"># Compute variances for each feature across patches in each batch</span>
        <span class="n">variances</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, z_dim)</span>
        
        <span class="c1"># Compute weights as inverse of variances</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">variances</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, z_dim)</span>
        
        <span class="c1"># Compute weighted sum of embeddings across patches for each batch</span>
        <span class="n">weighted_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, z_dim)</span>
        
        <span class="c1"># Compute robust mean by dividing by the sum of weights</span>
        <span class="n">robust_mean</span> <span class="o">=</span> <span class="n">weighted_sum</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, z_dim)</span>

        <span class="c1"># Pass the robust mean through the prediction head</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">robust_mean</span><span class="p">)</span>  <span class="c1"># Shape: (batch_size, n_out)</span>
        
        <span class="k">return</span> <span class="n">output</span></div>
</div>


<div class="viewcode-block" id="aodmil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.aodmil">[docs]</a>
<span class="k">class</span> <span class="nc">aodmil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attention-based Outlier Detection Multiple Instance Learning model. The model computes the mean embedding of the bag,</span>
<span class="sd">    computes the similarity of each instance to the mean embedding, and computes the attention weights based on the similarity.</span>
<span class="sd">    The final prediction is made using a linear classifier.</span>
<span class="sd">    The method aims to detect patches that show abnormal behavior with respect to the other patches in the bag.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability</span>
<span class="sd">    </span>
<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder : nn.Sequential</span>
<span class="sd">        Encoder network</span>
<span class="sd">    attention : nn.Sequential</span>
<span class="sd">        Attention network</span>
<span class="sd">    head : nn.Sequential</span>
<span class="sd">        Prediction head network</span>
<span class="sd">    </span>
<span class="sd">    Methods</span>
<span class="sd">    -------</span>
<span class="sd">    forward(bags)</span>
<span class="sd">        Forward pass through the model</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="aodmil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.aodmil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">aodmil</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">n_out</span><span class="p">)</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="aodmil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.aodmil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_patches</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bags</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        
        <span class="c1"># Compute the mean embedding of the bag</span>
        <span class="n">mean_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Mean along patches, retain batch dimension</span>
        
        <span class="c1"># Compute similarity of each instance to the mean embedding</span>
        <span class="n">similarities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">mean_embedding</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Inverse attention based on similarity</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">similarities</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">attention_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Weighted aggregation of embeddings</span>
        <span class="n">weighted_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">*</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">aggregated_embedding</span> <span class="o">=</span> <span class="n">weighted_embeddings</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Mean along patches, retain batch dimension</span>
        
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">aggregated_embedding</span><span class="p">)</span>  <span class="c1"># Shape: [batch_size, n_out]</span>
        <span class="k">return</span> <span class="n">output</span></div>
</div>


<div class="viewcode-block" id="clam_mil">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil">[docs]</a>
<span class="k">class</span> <span class="nc">clam_mil</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Clustering-constrained Attention Multiple Instance Learning (CLAM)</span>
<span class="sd">    model with attention-based pooling and instance-level clustering.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features.</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes.</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer.</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="clam_mil.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">clam_mil</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_out</span> <span class="o">=</span> <span class="n">n_out</span>
        
        <span class="c1"># Encoder Network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="c1"># Attention Network (shared backbone)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
        <span class="p">)</span>
        
        <span class="c1"># Class-specific Attention and Classification branches</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifiers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span>
        
        <span class="c1"># Clustering Layer for instance-level clustering</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_cluster</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span></div>


<div class="viewcode-block" id="clam_mil.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        
        <span class="n">slide_level_representations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">attention_weights_list</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Loop through each class-specific branch</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
            <span class="n">attention_U_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">attention_V_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">attention_U_output</span> <span class="o">*</span> <span class="n">attention_V_output</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attention_weights_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
            
            <span class="c1"># Slide-level representation</span>
            <span class="n">slide_level_representation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attention_scores</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">slide_level_representations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slide_level_representation</span><span class="p">)</span>

        <span class="c1"># Stack all slide-level representations</span>
        <span class="n">slide_level_representations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">slide_level_representations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Final classification scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
            <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifiers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">slide_level_representations</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        
        <span class="c1"># Aggregate scores into a tensor</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">attention_weights_list</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span></div>


<div class="viewcode-block" id="clam_mil.cluster_patches">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil.cluster_patches">[docs]</a>
    <span class="k">def</span> <span class="nf">cluster_patches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform instance-level clustering using pseudo-labels generated </span>
<span class="sd">        by the attention scores.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">bags</span><span class="p">)</span>
        <span class="n">cluster_predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
            <span class="n">cluster_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_cluster</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">cluster_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cluster_pred</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">cluster_predictions</span></div>
</div>


<div class="viewcode-block" id="clam_mil_mb">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb">[docs]</a>
<span class="k">class</span> <span class="nc">clam_mil_mb</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multi-Branch Clustering-constrained Attention Multiple Instance Learning (CLAM)</span>
<span class="sd">    model with attention-based pooling and instance-level clustering.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_feats : int</span>
<span class="sd">        Number of input features.</span>
<span class="sd">    n_out : int</span>
<span class="sd">        Number of output classes.</span>
<span class="sd">    z_dim : int</span>
<span class="sd">        Dimensionality of the hidden layer.</span>
<span class="sd">    dropout_p : float</span>
<span class="sd">        Dropout probability.</span>
<span class="sd">    n_branches : int</span>
<span class="sd">        Number of independent branches.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="clam_mil_mb.__init__">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb.__init__">[docs]</a>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_feats</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_out</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">n_branches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">clam_mil_mb</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_out</span> <span class="o">=</span> <span class="n">n_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_branches</span> <span class="o">=</span> <span class="n">n_branches</span>

        <span class="c1"># Separate encoder, attention, and classifier for each branch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_feats</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="c1"># Attention networks for each branch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="c1"># Class-specific Attention and Classification branches for each branch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifiers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="c1"># Clustering Layer for instance-level clustering for each branch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_cluster</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_out</span><span class="p">)])</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_branches</span><span class="p">)</span>
        <span class="p">])</span></div>


<div class="viewcode-block" id="clam_mil_mb.forward">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">,</span> <span class="n">return_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">all_slide_level_representations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_attention_weights</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Loop through each branch</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_branches</span><span class="p">):</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">[</span><span class="n">b</span><span class="p">](</span><span class="n">bags</span><span class="p">)</span>
            <span class="n">slide_level_representations</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">attention_weights_list</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># Loop through each class-specific branch</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
                <span class="n">attention_U_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_U</span><span class="p">[</span><span class="n">b</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
                <span class="n">attention_V_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_V</span><span class="p">[</span><span class="n">b</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
                <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_branches</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">i</span><span class="p">](</span><span class="n">attention_U_output</span> <span class="o">*</span> <span class="n">attention_V_output</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">attention_weights_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>

                <span class="c1"># Slide-level representation</span>
                <span class="n">slide_level_representation</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">attention_scores</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">slide_level_representations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slide_level_representation</span><span class="p">)</span>

            <span class="c1"># Stack slide-level representations for the current branch</span>
            <span class="n">slide_level_representations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">slide_level_representations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">all_slide_level_representations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slide_level_representations</span><span class="p">)</span>
            <span class="n">all_attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attention_weights_list</span><span class="p">)</span>

        <span class="c1"># Aggregate representations from all branches (e.g., by summing)</span>
        <span class="n">aggregated_representations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">all_slide_level_representations</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Final classification scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
            <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_branches</span><span class="p">):</span>
                <span class="n">score</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifiers</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">i</span><span class="p">](</span><span class="n">aggregated_representations</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="c1"># Aggregate scores into a tensor</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_attention</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">all_attention_weights</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span></div>


<div class="viewcode-block" id="clam_mil_mb.cluster_patches">
<a class="viewcode-back" href="../../../pathbench_models.html#pathbench.models.aggregators.clam_mil_mb.cluster_patches">[docs]</a>
    <span class="k">def</span> <span class="nf">cluster_patches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bags</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform instance-level clustering using pseudo-labels generated </span>
<span class="sd">        by the attention scores for each branch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_cluster_predictions</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_branches</span><span class="p">):</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoders</span><span class="p">[</span><span class="n">b</span><span class="p">](</span><span class="n">bags</span><span class="p">)</span>
            <span class="n">branch_cluster_predictions</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_out</span><span class="p">):</span>
                <span class="n">cluster_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">instance_cluster</span><span class="p">[</span><span class="n">b</span><span class="p">][</span><span class="n">i</span><span class="p">](</span><span class="n">embeddings</span><span class="p">)</span>
                <span class="n">branch_cluster_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cluster_pred</span><span class="p">)</span>
            <span class="n">all_cluster_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">branch_cluster_predictions</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">all_cluster_predictions</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Siemen Brussee.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>