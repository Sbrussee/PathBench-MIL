experiment:
  project_name: Exp1 # Name of the project
  annotation_file: /path/to/your/annotations # Path to the annotation file
  balancing: category # Balancing can be 'tile', 'slide', 'patient' or 'category'
  split_technique: k-fold # Split technique can be 'k-fold' or 'fixed'
  k: 3 # Number of folds for k-fold cross-validation
  val_fraction: 0.1 # Fraction of the training data to be used for validation
  best_epoch_based_on: val_loss # Metric to be used for selecting the best epoch, can be val_loss or any metric in evaluation
  epochs: 25 # Number of epochs for training
  batch_size: 32 # Batch size for training
  bag_size: 512 # Bag size for MIL models
  encoder_layers: 1 # Number of layers in the MIL encoder
  z_dim: 256 # Dimension of the latent space in the MIL encoder
  dropout_p: 0.1 # Dropout probability for the MIL model
  num_workers: 0 # Number of workers for data loading, 0 for no parallelization.
  aggregation_level: slide # Aggregation level can be 'slide' or 'patient'
  task: classification # Task can be 'classification', 'regression' or 'survival'
  mode: benchmark # Mode can be either 'benchmark' or 'optimization'.
  custom_metrics: [RocAuc] # Custom metrics to be used, needs to be defined in metrics.py or as a fastai-metric: https://docs.fast.ai/metrics.html
  report: False # Whether to generate a tile extraction report
  
  qc: # List of quality control methods to be used
    - GaussianV2
    - Otsu-CLAHE

  qc_filters: #Tile-level filters for discarding tiles
    grayspace_threshold : 0.05 #Pixels below this value (ranged 0-1) are considered gray.
    grayspace_fraction: 0.6 # Image tiles with grayspace above this fraction are discarded.
    whitespace_threshold: 230 #Pixel intensities (0-255) above this value are considered white.
    whitespace_fraction: 1.0 # Image tiles with whitespace above this fraction are discarded.

  evaluation: # Evaluation metrics to be used
    - balanced_accuracy
    - auc

  visualization: # Visualization methods to be used
    - confusion_matrix
    - roc_curve

optimization: # Optimization parameters
  study_name: Exp1 # Name of the optimization study
  load_study: True # Whether to load an existing study, which continues the optimization from a checkpoint
  objective_metric: balanced_accuracy # Objective metric to be optimized
  objective_mode: max # Optimization mode, can be 'max' or 'min'
  objective_dataset: test # Dataset to be used for the objective metric, can be 'val' or 'test'
  sampler: TPESampler # Sampler to be used, inherets from optuna.samplers
  pruner: HyperbandPruner # Pruner to be used, inherets from optuna.pruners
  trials: 20 # Number of trials to be run


datasets: # List of datasets to be used
  - name: training1 # Name of the dataset
    slide_path: path/to/your/slides # Path to the slide data
    tfrecord_path: path/to/your/tfrecords # Path to save tfrecords
    tile_path: path/to/save/tiles # Path to save tiles
    used_for: training # Whether the dataset is used for training or testing

  - name: testing1
    slide_path: path/to/your/slides
    tfrecord_path: path/to/your/tfrecords
    tile_path: path/to/save/tiles
    used_for: testing

benchmark_parameters:
  tile_px: # Tile sizes in pixels
    - 256 
  tile_um: # Magnifications (e.g. 40x, 20x, 10x)
    - 20x
  normalization: # Normalization methods, can be 'macenko', 'reinhard' or 'cyclegan'
    - macenko
    - reinhard
  feature_extraction: # Feature extraction methods, as specified in feature_extractors.py
    - resnet50_imagenet
    - CTransPath
    - RetCCL
    - virchow
    - uni

  mil: # MIL models, as specified in aggregators.py
    - clam_mil
    - dsmil
    - varmil
    - attention_mil

  loss: # Loss functions, as specified in losses.py
    - CrossEntropyLoss

  augmentation: # MIL-friendly augmentations, as specified in augmentations.py
    - patch_mixing

  activation_function: # activation function for the MIL encoder, supports any pytorch.nn activation function.
    - ReLU

  optimizer: # Optimizers, can be any fastai optimizer (Default: Adam)
    - Adam
  
weights_dir: ./pretrained_weights # Path to pretrained weights and/or where to save retrieved weights, defaults to the pretrained_weights directory in the PathDev repository
hf_key: YOUR_HUGGINGFACE_TOKEN # Token for Hugging Face model hub to access gated models, if you do not have one, just set to None